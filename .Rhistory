library(ggmap)
ggmap(get_map(bbox(geoT)))
ggmap(get_map(bbox(geoT))) + geom_point(data = geoT@data, aes(x = Lon, y = Lat), alpha = 0.2)
geoT@data[sample(1:nrow(geoT), size = nrow(geoT) / 1000), ]
ggmap(get_map(bbox(geoT))) + geom_point(data = geoT@data[sample(1:nrow(geoT), size = nrow(geoT) / 1000), ],
aes(x = Lon, y = Lat), alpha = 0.2)
names(geoT)
nrow(grep("robber", geoT$Text, ignore.case = T))
length(grep("robber", geoT$Text, ignore.case = T))
length(grep("stole|steal", geoT$Text, ignore.case = T))
head(geoT$Text[grep("robber", geoT$Text, ignore.case = T)])
length(grep("break-in|breakin", geoT$Text, ignore.case = T))
head(geoT$Text[grep("break-in|breakin", geoT$Text, ignore.case = T)])
head(geoT$Text[grep("break-in", geoT$Text, ignore.case = T)])
length(grep("break-in", geoT$Text, ignore.case = T))
length(grep("shoplift|shop-lift", geoT$Text, ignore.case = T))
head(geoT$Text[grep("shoplift|shop-lift", geoT$Text, ignore.case = T)])
length(grep("thei", geoT$Text, ignore.case = T))
head(geoT$Text[grep("thei", geoT$Text, ignore.case = T)])
length(grep("burgl", geoT$Text, ignore.case = T))
head(geoT$Text[grep("burgl", geoT$Text, ignore.case = T)])
head(geoT$Text[grep("thei(fv)", geoT$Text, ignore.case = T)])
head(geoT$Text[grep("thei(f|v)", geoT$Text, ignore.case = T)])
length(grep("robber", geoT$Text, ignore.case = T))
head(geoT$Text[grep("robber", geoT$Text, ignore.case = T)])
length(grep("burgl", geoT$Text, ignore.case = T))
head(geoT$Text[grep("burgl", geoT$Text, ignore.case = T)])
length(grep("stole|steal", geoT$Text, ignore.case = T))
head(geoT$Text[grep("stole|steal", geoT$Text, ignore.case = T)])
length(grep("shoplift|shop-lift", geoT$Text, ignore.case = T))
head(geoT$Text[grep("shoplift|shop-lift", geoT$Text, ignore.case = T)])
length(grep("thei", geoT$Text, ignore.case = T))
head(geoT$Text[grep("thei(f|v)", geoT$Text, ignore.case = T)])
length(grep("thei(f|v)", geoT$Text, ignore.case = T))
selection <- grep("robber|burgl|stole|steal|shoplift|shop-lift|thei(f|v)", geoT$Text, ignore.case = T)
crimeTweets <- geoT[selection,]
nrow(crimeTweets)
library(maptools)
x <- spRbind(crimeTweets, geoT[sample(1:nrow(geoT), size = nrow(crimeTweets)), ])
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1)
x <- geoT[sample(1:nrow(geoT), size = nrow(crimeTweets)), ]
crimeTweets$type <- "Crime"
set.seed(2014) # reproducible starting point for random numbers
crimeTweets$type <- "Crime"
x <- geoT[sample(1:nrow(geoT), size = nrow(crimeTweets)), ]
x$type <- "Baseline"
x <- spRbind(crimeTweets, x)
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1) +
facet_grid(~ type)
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1) +
geom_density2d(fill = ..density..)
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1) +
geom_density2d(aes(fill = ..density..))
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1) +
geom_density2d(aes(fill = ..level..))
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1) +
geom_density2d(aes(fill = ..level..), geom = "polygon")
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1) +
stat_density2d(aes(fill = ..level..), geom = "polygon")
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1) +
stat_density2d(aes(fill = ..level.., alpha=..level..), geom = "polygon") +
facet_grid(~ type)
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1) +
stat_density2d(aes(fill = ..level.., alpha=..level..), geom = "polygon") +
facet_grid(~ type) + scale_fill_continuous(low = "green", high = "red")
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1) +
stat_density2d(aes(fill = ..level..), alpha = 0.3, geom = "polygon") +
facet_grid(~ type) + scale_fill_continuous(low = "green", high = "red")
write.csv(geoT@data, "~/Dropbox/Public/crimeTweets.csv")
write.csv(crimeTweets@data, "~/Dropbox/Public/crimeTweets.csv")
length(grep("pennine way", geoT$Text, ignore.case = T))
head(geoT$Text[grep("pennine way", geoT$Text, ignore.case = T)])
length(grep("pennine", geoT$Text, ignore.case = T))
head(geoT$Text[grep("pennine", geoT$Text, ignore.case = T)])
download.file(url = "http://hiking.waymarkedtrails.org/en/routebrowser/63872/gpx")
download.file(url = "http://hiking.waymarkedtrails.org/en/routebrowser/63872/gpx", "pennine.gpx")
source('~/.active-rstudio-document', echo=TRUE)
download.file(url = "http://hiking.waymarkedtrails.org/en/routebrowser/63872/gpx", "pennine.gpx")
library(rgdal)
ogrListLayers("pennine.gpx")
pw <- readOGR("pennine.gpx", layer = "routes")
pw <- readOGR("pennine.gpx", layer = "tracks")
library(rgeos)
gBuffer(pw, width = 0.3)
pwBuf <- gBuffer(pw, width = 0.3)
plot(pwBuf)
pw <- spTransform(pw, CRS("+init=epsg:27700"))
library(rgeos)
pwBuf <- gBuffer(pw, width = 2000)
plot(pwBuf)
pwBuf <- gBuffer(pw, width = 5000)
pwBuf <- spTransform(pwBuf, CRS("+init=epsg:4326"))
geoT[pwBuf, ]
pwBuf <- spTransform(pwBuf, CRS(proj4string(geoT)))
pwBuf <- spTransform(pwBuf, CRS("+init=epsg:4326"))
proj4string(geoT) <- CRS("+init=epsg:4326")
geoT[pwBuf, ]
pwBuf <- gBuffer(pw, width = 20000) # create 5 km buffer
plot(pwBuf) # plot to test dimensions make sense
pwBuf <- spTransform(pwBuf, CRS("+init=epsg:4326"))
proj4string(geoT) <- CRS("+init=epsg:4326")
geoT[pwBuf, ]
pwBuf <- gBuffer(pw, width = 10000) # create 10 km buffer
plot(pwBuf) # plot to test dimensions make sense
pwBuf <- spTransform(pwBuf, CRS("+init=epsg:4326"))
proj4string(geoT) <- CRS("+init=epsg:4326")
PennineTweets <- geoT[pwBuf, ]
nrow(PennineTweets)
pwBuf <- gBuffer(pw, width = 15000) # create 10 km buffer
plot(pwBuf) # plot to test dimensions make sense
pwBuf <- spTransform(pwBuf, CRS("+init=epsg:4326"))
proj4string(geoT) <- CRS("+init=epsg:4326")
PennineTweets <- geoT[pwBuf, ]
nrow(PennineTweets)
points(PennineTweets)
plot(pwBuf)
points(PennineTweets)
pTweets <- geoT[ grep("pennine", geoT$Text, ignore.case = T) , ]
write.csv(pTweets, "~/Dropbox/Public/tmp/ptweets.csv")
load("../tweet_store/.RData")
install.packages("RSQLite")
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1) +
stat_density2d(aes(fill = ..level..), alpha = 0.3, geom = "polygon") +
facet_grid(~ type) + scale_fill_continuous(low = "green", high = "red")
set.seed(2014) # reproducible starting point for random numbers
crimeTweets$type <- "Crime"
x <- geoT[sample(1:nrow(geoT), size = nrow(crimeTweets)), ]
x$type <- "Baseline"
selection <- grep("robber|burgl|stole|steal|shoplift|shop-lift|thei(f|v)", geoT$Text, ignore.case = T)
crimeTweets <- geoT[selection,]
nrow(crimeTweets)
library(maptools) # load package
set.seed(2014) # reproducible starting point for random numbers
crimeTweets$type <- "Crime"
x <- geoT[sample(1:nrow(geoT), size = nrow(crimeTweets)), ]
x$type <- "Baseline"
x <- spRbind(crimeTweets, x)
ggplot(data = x@data, aes(x = Lon, y = Lat)) + geom_point(alpha = 0.1) +
stat_density2d(aes(fill = ..level..), alpha = 0.3, geom = "polygon") +
facet_grid(~ type) + scale_fill_continuous(low = "green", high = "red")
library(tm)
?tm
vc <- tm::as.VCorpus(crimeTweets$Text)
vc <- VectorSource(crimeTweets$Text)
vc <- VCorpus(VectorSource(crimeTweets$Text))
vc <- tm_map(vc, content_transformer(tolower))
dtm <- DocumentTermMatrix(vc)
findFreqTerms(dtm, 20)
findFreqTerms(dtm, 50)
findFreqTerms(dtm, 100)
findFreqTerms(dtm, 100)[ nchar(findFreqTerms(dtm, 100)) > 5]
findFreqTerms(dtm, 50)[ nchar(findFreqTerms(dtm, 50)) > 5]
findAssocs(x = vc, terms = "steal", corlimit = 0.5)
findAssocs(x = dtm, terms = "steal", corlimit = 0.5)
findAssocs(x = dtm, terms = "steal", corlimit = 0.3)
findAssocs(x = dtm, terms = "steal", corlimit = 0.2)
findAssocs(x = dtm, terms = "steal", corlimit = 0.1)
findAssocs(x = dtm, terms = "burgl", corlimit = 0.1)
findAssocs(x = dtm, terms = "burgle", corlimit = 0.1)
findAssocs(x = dtm, terms = "burgl*", corlimit = 0.1)
findAssocs(x = dtm, terms = "burglary", corlimit = 0.1)
head(findAssocs(x = dtm, terms = "theif", corlimit = 0.1))
head(findAssocs(x = dtm, terms = "robbery", corlimit = 0.1))
head(findAssocs(x = dtm, terms = "steal", corlimit = 0.1))
head(findAssocs(x = dtm, terms = "burgle", corlimit = 0.1))
head(findAssocs(x = dtm, terms = "burglary", corlimit = 0.1))
head(findAssocs(x = dtm, terms = "shoplift", corlimit = 0.1))
head(findAssocs(x = dtm, terms = "theif", corlimit = 0.1))
head(findAssocs(x = dtm, terms = "police", corlimit = 0.1))
?stopwords
stopwords()
vc <- tm_map(vc, removeWords, stopwords(kind = "en")) # remove 'stopwords' we're not interested in
dtm <- DocumentTermMatrix(vc)
findFreqTerms(dtm, 50)[ nchar(findFreqTerms(dtm, 50)) > 5] # words more than 5 letters long appearing 50+ times
findFreqTerms(dtm, 30)[ nchar(findFreqTerms(dtm, 30)) > 5] # words more than 5 letters long appearing 50+ times
head(findAssocs(x = dtm, terms = "robbery", corlimit = 0.1))
head(findAssocs(x = dtm, terms = "steal", corlimit = 0.1))
head(findAssocs(x = dtm, terms = "burgle", corlimit = 0.1))
tweets <- fromJSON(sprintf("[%s]", paste(readLines("data/tweets-large1.json"),collapse=",")))
library(rjson)
tweets <- fromJSON(sprintf("[%s]", paste(readLines("data/tweets-large1.json"),collapse=",")))
names(tweet) # show the list names - what data has been loaded
names(tweet) # show the list names - what data has been loaded
coords <- matrix(unlist(sapply(ts, function(x) x$coordinates$coordinates )), ncol=2, byrow=T)
head(coords)
nrow(coords)
coords <- matrix(unlist(sapply(tweets, function(x) x$coordinates$coordinates )), ncol=2, byrow=T)
head(coords)
nrow(coords)
text <- sapply(ts, function(x) x$text )
text <- unlist(text)[-1]
text[1:3]
text <- sapply(ts, function(x) x$text )
head(text)
text <- unlist(text)[-1]
head(text)
text <- sapply(ts, function(x) x$text )
class(text)
text[5]
text[1]
text <- unlist(text)[-1]
text[1]
text[1]
text[5]
text <- sapply(ts, function(x) x$text )
text <- unlist(text)
text <- sapply(ts, function(x) x$text )
text <- unlist(text)
text <- sapply(ts, function(x) x$text )
text <- unlist(text)
text <- sapply(ts, function(x) x$text )
text1 <- unlist(text)
identical(text, text1)
text <- sapply(tweets, function(x) x$text )
names(tweet) # show the list names - what data has been loaded
created <- sapply(tweets, function(x) x$created_at)
class(created)
head(created)
created <- strptime(sapply(tweets, function(x) x$created_at), "%a %b %d %H:%M:%S %Y")
head(created)
created <- sapply(tweets, function(x) x$created_at)
strptime(head(created) "%a %b %d %H:%M:%S %Y")
strptime(head(created), "%a %b %d %H:%M:%S %Y")
?strptime
strptime(head(created), "%a %b %d %H:%M:%S +0000 %Y")
head(created)
strptime(head(created), "%a %b %d %H:%M:%S +0000 %Y")
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
class(created)
plot(created)
plot(created, coord[,1])
plot(created, coords[,1])
plot(created, coords[,1])
nrow(coords)
length(created)
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
coords <- matrix(unlist(coords), ncol=2, byrow=T)
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
coords <- unlist(coords)
ts[[1]]$coordinates$coordinates # the coordinates
sapply(ts, function(x) x$coordinates$coordinates )[[1]] # the extracted coords
coords <- matrix(unlist(sapply(ts, function(x) x$coordinates$coordinates )), ncol=2, byrow=T)
coords <- as.data.frame(coords)
nrow(coords)
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
head(coords[[1]])
str(coords)
coords <- unlist(coords, recursive = T)
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
coords <- as.numeric(coords)
coords <- lapply(coords, as.numeric)
coords <- unlist(coords, recursive = T)
coords[[2]]
coords[[1]]
coords[[1]]
coords[[1:2]]
coords[[1]]
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
coords[[1]]
coords[[2]]
coords <- lapply(coords, as.numeric)
coords[[1]]
coords <- unlist(coords, recursive = T)
coords <- lapply(coords, function(x) if( x == NULL) {x <- c(0,0) })
coords[[1]]
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
coords[[1]]
coords <- lapply(coords, function(x) if( x == NULL) {x <- c(0,0) })
coords[[1]]
if( x == NULL) {x <- c(0,0) }
?if
()0
if( x == NULL) x <- c(0,0)
(x <- coords[[1]])
if( x == NULL) x <- c(0,0)
x == NULL
x
if( is.null(x)) x <- c(0,0)
x
coords <- lapply(coords, function(x) if(is.null(x)) {x <- c(0,0) })
coords <- unlist(coords, recursive = T)
head(coord)
head(coords)
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
coords <- lapply(coords, function(x) if(is.null(x)) {x <- c(0,0) })
coords <- unlist(coords, recursive = T)
head(coords)
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
coords[[2]]
coords[[1]]
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
coords <- sapply(coords, function(x) if(is.null(x)) {x <- c(0,0) })
coords <- unlist(coords, recursive = T)
coords <- sapply(coords, as.numeric)
coords <- unlist(coords, recursive = T)
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
coords <- sapply(coords, as.numeric)
coords <- unlist(coords, recursive = T)
text <- sapply(tweets, function(x) x$text )
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
coords <- sapply(coords, function(x) if(is.null(x)) {x <- c(0,0) })
coords <- unlist(coords, recursive = T)
nuls <- sapply(input.list, function(x) is.null(x))
nuls <- sapply(coords, function(x) is.null(x))
head(nuls)
summary(nuls)
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x))
summary(nuls)
summary(coords[nuls])
coords[nuls] <- c(0, 0)
lapply(coords[nuls], function(x) x <- c(0, 0))
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0))
coords <- unlist(coords, recursive = T)
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x))
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0))
coords <- unlist(coords, recursive = T)
class(coords)
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0))
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
plot(created, coords[,1])
t.out <- data.frame(text, lat = coords[,1], lon = coords[,2], created)
sel <- grepl("new house|new flat|moving house|move house|moving home|move home|#newhouse|#newflat|#movinghouse|#movehouse|#movinghome|#movehome",
t.out$text)
summary(sel)
t.out[sel,]
sel <- grepl("new house|new flat|moving house|move house|moving home|move home|#newhouse|#newflat|#movinghouse|#movehouse|#movinghome|#movehome", t.out$text, ignore.case = T )
summary(sel)
t.out[sel,]
t_out <- data.frame(text, lat = coords[,1], lon = coords[,2], created)
write.csv(t_out[sel, ], "house_tweets.csv")
sel <- grep("new house|new flat|moving house|move house|moving home|move home|#newhouse|#newflat|#movinghouse|#movehouse|#movinghome|#movehome", t.out$text, ignore.case = T )
library(rjson) # library used to load .json files
files <- list.files(path = "testload/", full.names=T)
for(i in files){
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i), collapse=",")))
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text <- sapply(tweets, function(x) x$text )
created <- strptime(sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created)
sel <- grepl("new house|new flat|moving house|move house|moving home|move home|#newhouse|#newflat|#movinghouse|#movehouse|#movinghome|#movehome", t.out$text, ignore.case = T )
filenum <- paste0(which(files == i), ".csv")
write.csv(t_out[sel, ], "output.csv", append = T)
}
i <- files[1]
i
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i), collapse=",")))
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text <- sapply(tweets, function(x) x$text )
created <- strptime(sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
created <- strptime(sapply(tweets, function(x) x$created_at))
created <- sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created)
sel <- grepl("new house|new flat|moving house|move house|moving home|move home|#newhouse|#newflat|#movinghouse|#movehouse|#movinghome|#movehome", t.out$text, ignore.case = T )
sel <- grepl("new house|new flat|moving house|move house|moving home|move home|#newhouse|#newflat|#movinghouse|#movehouse|#movinghome|#movehome", t_out$text, ignore.case = T )
which(files == i)
t_out$filenum <- which(files == i)
write.csv(t_out[sel, ], "output.csv", append = T)
??append
write.csv(t_out[sel, ], file = "output.csv", append = T)
write.csv(t_out[sel, ], file = "output.csv", append = T)
sel <- grepl("a", t_out$text, ignore.case = T )
t_out$filenum <- which(files == i)
write.csv(t_out[sel, ], file = "output.csv", append = T)
sel <- grepl("x", t_out$text, ignore.case = T )
t_out$filenum <- which(files == i)
write.csv(t_out[sel, ], file = "output.csv", append = T)
write.table(t_out[sel, ], sep=",", file = "output.csv", append = T)
write.table(t_out[sel, ], sep=",", file = "output.csv", append = T)
write.table(t_out[sel, ], sep=",", file = "output.csv", append = T)
write.table(t_out[sel, ], sep=",", file = "output.csv", append = T, )
write.csv(t_out[sel, ], sep=",", file = paste0("output",which(files == i),".csv"), append = T, )
write.csv(t_out[sel, ], file = paste0("output",which(files == i),".csv"))
read.csv("output1.csv")
library(rjson) # library used to load .json files
files <- list.files(path = "testload/", full.names=T)
for(i in files){
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i, n=10), collapse=",")))
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text <- sapply(tweets, function(x) x$text )
created <- sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created)
# sel <- grepl("new house|new flat|moving house|move house|moving home|move home|#newhouse|#newflat|#movinghouse|#movehouse|#movinghome|#movehome", t_out$text, ignore.case = T )
sel <- grepl("x", t_out$text, ignore.case = T )
t_out$filenum <- which(files == i)
write.csv(t_out[sel, ], file = paste0("output",which(files == i),".csv"))
}
library(rjson) # library used to load .json files
files <- list.files(path = "testload/", full.names=T)
for(i in files){
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i, n=10), collapse=",")))
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text <- sapply(tweets, function(x) x$text )
created <- sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created)
# sel <- grepl("new house|new flat|moving house|move house|moving home|move home|#newhouse|#newflat|#movinghouse|#movehouse|#movinghome|#movehome", t_out$text, ignore.case = T )
sel <- grepl("a", t_out$text, ignore.case = T )
t_out$filenum <- which(files == i)
write.csv(t_out[sel, ], file = paste0("output",which(files == i),".csv"))
}
library(microbenchmark)
install.packages("microbenchmark")
library(microbenchmark)
start_time <- system.time()
start_time <- Sys.time()
start_time
end_time <- Sys.time()
end_time <- Sys.time()
(time_taken <- end_time - start_time)
# BigLoad.R - for loading large amounts of twitter data into R
# Unzip the files saved by tweepy (see https://github.com/Robinlovelace/tweepy)
# Save to "unzipped" (e.g. with gunzip), load these files
library(microbenchmark)
library(rjson) # library used to load .json files
files <- list.files(path = "data/unzipped/", full.names=T)
# i <- files[1] # uncomment to load 1
start_time <- Sys.time()
for(i in files){
# tweets <- fromJSON(sprintf("[%s]", paste(readLines(i, n=1000), collapse=","))) # test subset
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i, 1000), collapse=","))) # full dataset
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text <- sapply(tweets, function(x) x$text )
created <- sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
language <- sapply(tweets, function(x) x$lang )
n_followers <- sapply(tweets, function(x) x$lang )
user_created <- sapply(tweets, function(x) x$user$created_at)
n_tweets <- sapply(tweets, function(x) x$user$statuses_count)
n_followers <- sapply(tweets, function(x) x$user$followers_count)
n_following <- sapply(tweets, function(x) x$user$friends_count)
user_location <- sapply(tweets, function(x) x$user$location)
user_description <- sapply(tweets, function(x) x$user$description)
user_id <- sapply(tweets, function(x) x$user$id)
user_idstr <- sapply(tweets, function(x) x$user$id_str)
user_name <- sapply(tweets, function(x) x$user$name)
user_screen_name <- sapply(tweets, function(x) x$user$screen_name)
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created,
language, n_followers, user_created, n_tweets, n_followers, n_following,
user_location)
# sel <- grepl("new house|new flat|moving house|move house|moving home|move home|#newhouse|#newflat|#movinghouse|#movehouse|#movinghome|#movehome", t_out$text, ignore.case = T ) # original selection
# sel <- grepl("a", t_out$text, ignore.case = T ) # test selection - replace "a" with anything
sel <- grepl("new house|#newhouse|old house|#oldhouse|new home|#newhome|old home|#oldhome|new flat|#newflat|old flat|#oldflat|moving house|#movinghouse|move house|#movehouse|moving home|#movinghome|move home|#movehome|packing to move|packing up everything|unpacking everything|removals van|#packingtomove|#packingupeverything|#unpackingeverything|#removalsvan|bought a house|house bought|moved house|house sold|#boughtahouse|#housebought|#movedhouse|#housesold|first rent|#firstrent|new gaff|new housing|new accommodation|new crib|new bungalow|new apartment|new semi detached|new semi-detached|new detached|new cottage|new digs|new dwelling|new residence|new pad|new homes|new home's|new houses|new house's|#newgaff|#newhousing|#newaccommodation|#newcrib|#newbungalow|#newapartment|#newsemidetached|#newdetached|#newcottage|#newdigs|#newdwelling|#newresidence|#newpad|#newhomes|#newhouses|old gaff|old housing|old accommodation|old crib|old bungalow|old apartment|old semi detached|old semi-detached|old detached|old cottage|old digs|old dwelling|old residence|old pad|old homes|old home's|old houses|old house's|#oldgaff|#oldhousing|#oldaccommodation|#oldcrib|#oldbungalow|#oldapartment|#oldsemidetached|#olddetached|#oldcottage|#olddigs|#olddwelling|#oldresidence|#oldpad|#oldhomes|#oldhouses", t_out$text, ignore.case = T)
t_out$filenum <- which(files == i)
write.csv(t_out[sel, ], file = paste0("data/output",which(files == i),".csv"))
}
end_time <- Sys.time()
(time_taken <- end_time - start_time)
# BigLoad.R - for loading large amounts of twitter data into R
# Unzip the files saved by tweepy (see https://github.com/Robinlovelace/tweepy)
# Save to "unzipped" (e.g. with gunzip), load these files
library(microbenchmark)
library(rjson) # library used to load .json files
files <- list.files(path = "data/unzipped/", full.names=T)
# i <- files[1] # uncomment to load 1
start_time <- Sys.time()
for(i in files){
# tweets <- fromJSON(sprintf("[%s]", paste(readLines(i, n=1000), collapse=","))) # test subset
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i), collapse=","))) # full dataset
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text <- sapply(tweets, function(x) x$text )
created <- sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
language <- sapply(tweets, function(x) x$lang )
n_followers <- sapply(tweets, function(x) x$lang )
user_created <- sapply(tweets, function(x) x$user$created_at)
n_tweets <- sapply(tweets, function(x) x$user$statuses_count)
n_followers <- sapply(tweets, function(x) x$user$followers_count)
n_following <- sapply(tweets, function(x) x$user$friends_count)
user_location <- sapply(tweets, function(x) x$user$location)
user_description <- sapply(tweets, function(x) x$user$description)
user_id <- sapply(tweets, function(x) x$user$id)
user_idstr <- sapply(tweets, function(x) x$user$id_str)
user_name <- sapply(tweets, function(x) x$user$name)
user_screen_name <- sapply(tweets, function(x) x$user$screen_name)
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created,
language, n_followers, user_created, n_tweets, n_followers, n_following,
user_location)
# sel <- grepl("new house|new flat|moving house|move house|moving home|move home|#newhouse|#newflat|#movinghouse|#movehouse|#movinghome|#movehome", t_out$text, ignore.case = T ) # original selection
# sel <- grepl("a", t_out$text, ignore.case = T ) # test selection - replace "a" with anything
sel <- grepl("new house|#newhouse|old house|#oldhouse|new home|#newhome|old home|#oldhome|new flat|#newflat|old flat|#oldflat|moving house|#movinghouse|move house|#movehouse|moving home|#movinghome|move home|#movehome|packing to move|packing up everything|unpacking everything|removals van|#packingtomove|#packingupeverything|#unpackingeverything|#removalsvan|bought a house|house bought|moved house|house sold|#boughtahouse|#housebought|#movedhouse|#housesold|first rent|#firstrent|new gaff|new housing|new accommodation|new crib|new bungalow|new apartment|new semi detached|new semi-detached|new detached|new cottage|new digs|new dwelling|new residence|new pad|new homes|new home's|new houses|new house's|#newgaff|#newhousing|#newaccommodation|#newcrib|#newbungalow|#newapartment|#newsemidetached|#newdetached|#newcottage|#newdigs|#newdwelling|#newresidence|#newpad|#newhomes|#newhouses|old gaff|old housing|old accommodation|old crib|old bungalow|old apartment|old semi detached|old semi-detached|old detached|old cottage|old digs|old dwelling|old residence|old pad|old homes|old home's|old houses|old house's|#oldgaff|#oldhousing|#oldaccommodation|#oldcrib|#oldbungalow|#oldapartment|#oldsemidetached|#olddetached|#oldcottage|#olddigs|#olddwelling|#oldresidence|#oldpad|#oldhomes|#oldhouses", t_out$text, ignore.case = T)
t_out$filenum <- which(files == i)
write.csv(t_out[sel, ], file = paste0("data/output",which(files == i),".csv"))
}
end_time <- Sys.time()
(time_taken <- end_time - start_time)
memory.limit()
